#!/usr/bin/env python3
"""
H-optimus-0 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (ë©”ëª¨ë¦¬ ìµœì í™”)
"""
import os
import sys
import gc

hf_token = os.environ.get('HF_TOKEN')

if not hf_token:
    print("âŒ ì˜¤ë¥˜: HF_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    sys.exit(1)

try:
    from huggingface_hub import login
    print('ğŸ”‘ HuggingFace ë¡œê·¸ì¸ ì¤‘...')
    login(token=hf_token)
    print('âœ… ë¡œê·¸ì¸ ì„±ê³µ!')
except Exception as e:
    print('âš ï¸ ë¡œê·¸ì¸ ì‹¤íŒ¨ (ì´ë¯¸ ë¡œê·¸ì¸ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ): ' + str(e))

try:
    import torch
    import timm
    
    torch.set_grad_enabled(False)
    
    print('ğŸ“¦ H-optimus-0 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ë©”ëª¨ë¦¬ ìµœì í™”)')
    from huggingface_hub import snapshot_download
    cache_dir = snapshot_download(
        repo_id='bioptimus/H-optimus-0',
        token=hf_token,
        local_files_only=False
    )
    print('âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ! (ìºì‹œ ê²½ë¡œ: ' + str(cache_dir) + ')')
    
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
except Exception as e:
    print('âš ï¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì´ë¯¸ ìºì‹œì— ìˆì„ ìˆ˜ ìˆìŒ): ' + str(e))
    print('ğŸ’¡ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...')
