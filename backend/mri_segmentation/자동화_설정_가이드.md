# 연구실 컴퓨터 자동 추론 설정 가이드

## ✅ 완료된 기능

**프론트엔드에서 "AI 분석" 버튼을 누르면 자동으로 연구실 컴퓨터에서 추론이 실행됩니다!**

프론트엔드 코드 변경 없이, Django 백엔드가 자동으로 연구실 컴퓨터 워커를 사용하도록 설정되었습니다.

---

## 🔄 동작 방식

### 전체 흐름

```
1. [프론트엔드] 사용자가 "AI 분석" 버튼 클릭
   ↓
2. [Django] /api/mri/segmentation/series/{series_id}/segment/ API 호출
   ↓
3. [Django] 환경 변수 확인 (USE_LOCAL_INFERENCE)
   ↓
4. [Django] 연구실 컴퓨터 요청 생성 (/tmp/mri_inference_requests/)
   ↓
5. [연구실 컴퓨터 워커] 30초마다 요청 확인 (자동)
   ↓
6. [연구실 컴퓨터 워커] 요청 발견 → 자동 처리
   ↓
7. [연구실 컴퓨터 워커] Orthanc에서 DICOM 다운로드 → 추론 → 업로드
   ↓
8. [Django] 요청 상태 폴링 (최대 5분 대기)
   ↓
9. [Django] 완료 확인 → 결과 반환
   ↓
10. [프론트엔드] 결과 표시
```

---

## 🚀 설정 방법

### 1단계: 연구실 컴퓨터에서 워커 실행

**연구실 컴퓨터에서:**

```bash
cd ~/연구실_컴퓨터_추론_패키지/mri_segmentation

# 가상환경 활성화
source venv/bin/activate

# 워커 실행 (백그라운드)
nohup python local_inference_worker.py > worker.log 2>&1 &

# 또는 systemd 서비스로 실행 (권장)
sudo systemctl start mri-inference-worker
sudo systemctl enable mri-inference-worker
```

**워커가 실행 중인지 확인:**
```bash
# 프로세스 확인
ps aux | grep local_inference_worker

# 로그 확인
tail -f worker.log

# systemd 서비스 확인
sudo systemctl status mri-inference-worker
```

### 2단계: GCP Django 서버 설정

**GCP 서버에서:**

```bash
# 환경 변수 설정 (.env 또는 systemd 환경 변수)
export USE_LOCAL_INFERENCE=true

# 또는 Django settings.py에 추가
# USE_LOCAL_INFERENCE = os.getenv('USE_LOCAL_INFERENCE', 'false').lower() == 'true'

# 요청 디렉토리 확인 (연구실 컴퓨터와 공유 가능해야 함)
# 기본값: /tmp/mri_inference_requests/
# NFS 마운트 또는 공유 스토리지 사용 권장

# Django 재시작
sudo systemctl restart gunicorn
```

**또는 NFS 마운트 설정 (권장):**

```bash
# 연구실 컴퓨터에서 NFS 서버 실행 (선택사항)
# 또는 공유 디렉토리 사용

# GCP 서버에서 마운트
sudo mount -t nfs 연구실컴퓨터IP:/shared/inference_requests /tmp/mri_inference_requests
```

---

## ⚙️ 환경 변수 설정

### 연구실 컴퓨터 (.env 파일)

```bash
ORTHANC_URL=http://34.42.223.43:8042
ORTHANC_USER=admin
ORTHANC_PASSWORD=admin123
MODEL_PATH=src/best_model.pth
DEVICE=cuda
THRESHOLD=0.5
REQUEST_DIR=/tmp/mri_inference_requests  # Django와 공유 디렉토리
POLL_INTERVAL=30  # 30초마다 요청 확인
```

### GCP Django 서버

```bash
# .env 파일 또는 systemd 환경 변수
USE_LOCAL_INFERENCE=true  # 연구실 컴퓨터 사용
# 또는
USE_LOCAL_INFERENCE=false  # GCP에서 직접 실행 (기존 방식)

INFERENCE_REQUEST_DIR=/tmp/mri_inference_requests  # 요청 디렉토리
```

---

## 🔧 동작 모드

### 모드 1: 자동 감지 (기본)

Django가 환경 변수 `USE_LOCAL_INFERENCE`를 확인하여 자동으로 선택:

```bash
# 연구실 컴퓨터 사용
export USE_LOCAL_INFERENCE=true

# GCP에서 직접 실행
export USE_LOCAL_INFERENCE=false
```

### 모드 2: API 파라미터로 제어

프론트엔드에서 강제로 선택 가능 (현재는 사용 안 함):

```typescript
// 연구실 컴퓨터 사용
fetch(`/api/mri/segmentation/series/${seriesId}/segment/?use_local=true`, ...)

// GCP에서 직접 실행
fetch(`/api/mri/segmentation/series/${seriesId}/segment/?force_gcp=true`, ...)
```

---

## 📊 성능 및 대기 시간

### 처리 시간

| 단계 | 시간 |
|------|------|
| 요청 생성 | 즉시 |
| 워커 감지 | 최대 30초 (폴링 간격) |
| DICOM 다운로드 | 2-5분 |
| 추론 실행 | 30-60초 (GPU) / 10-20분 (CPU) |
| Orthanc 업로드 | 2-3초 |
| **총 소요 시간** | **약 3-7분 (GPU)** / **12-25분 (CPU)** |

### Django 대기 시간

- Django는 최대 **5분(300초)** 동안 워커의 완료를 기다립니다
- 5분 내에 완료되지 않으면 타임아웃 응답을 반환하지만, 요청은 계속 처리됩니다
- 프론트엔드에서 상태 API로 나중에 확인 가능

---

## ✅ 체크리스트

### 연구실 컴퓨터
- [ ] 워커 실행 중 (`local_inference_worker.py`)
- [ ] 로그에서 정상 작동 확인
- [ ] 요청 디렉토리 접근 가능 (`/tmp/mri_inference_requests/`)
- [ ] Orthanc 서버 접근 가능
- [ ] 모델 파일 존재 (`src/best_model.pth`)

### GCP Django 서버
- [ ] `USE_LOCAL_INFERENCE=true` 설정
- [ ] 요청 디렉토리 접근 가능 (연구실 컴퓨터와 공유)
- [ ] Django 재시작 완료

### 테스트
- [ ] 프론트엔드에서 "AI 분석" 버튼 클릭
- [ ] 요청 파일 생성 확인 (`/tmp/mri_inference_requests/*.json`)
- [ ] 워커가 요청 처리 시작 확인 (로그)
- [ ] 추론 완료 확인
- [ ] 프론트엔드에 결과 표시 확인

---

## 🔍 문제 해결

### Q1: 워커가 요청을 처리하지 않음

**확인 사항:**
```bash
# 1. 워커 실행 확인
ps aux | grep local_inference_worker

# 2. 요청 파일 확인
ls -lh /tmp/mri_inference_requests/

# 3. 로그 확인
tail -f worker.log

# 4. 요청 디렉토리 권한 확인
ls -ld /tmp/mri_inference_requests/
```

**해결:**
- 워커 재시작
- 요청 디렉토리 권한 수정: `chmod 777 /tmp/mri_inference_requests/`
- NFS 마운트 확인 (공유 디렉토리 사용 시)

### Q2: Django가 타임아웃 응답

**원인:**
- 워커가 실행되지 않음
- 추론 시간이 5분 초과
- 네트워크 문제

**해결:**
```bash
# 1. 워커 상태 확인
sudo systemctl status mri-inference-worker

# 2. 요청 상태 확인 (나중에)
curl http://localhost:8000/api/mri/segmentation/status/{request_id}/

# 3. 타임아웃 시간 증가 (필요시)
# segmentation_views.py의 max_wait_time 수정
```

### Q3: 요청 파일이 생성되지 않음

**확인:**
```bash
# Django 로그 확인
sudo journalctl -u gunicorn -f

# 요청 디렉토리 확인
ls -ld /tmp/mri_inference_requests/
```

**해결:**
- 요청 디렉토리 생성: `mkdir -p /tmp/mri_inference_requests/`
- 권한 설정: `chmod 777 /tmp/mri_inference_requests/`

---

## 💡 팁

### 1. 프로덕션 환경

**systemd 서비스 사용 (권장):**
```bash
# 연구실 컴퓨터에서
sudo systemctl enable mri-inference-worker
sudo systemctl start mri-inference-worker

# 자동 재시작 설정
# systemd 서비스 파일에 Restart=always 포함됨
```

### 2. 모니터링

**워커 로그 모니터링:**
```bash
# 실시간 로그
tail -f worker.log

# systemd 로그
sudo journalctl -u mri-inference-worker -f
```

**요청 상태 모니터링:**
```bash
# Django API로 확인
curl http://localhost:8000/api/mri/segmentation/requests/

# 특정 요청 상태
curl http://localhost:8000/api/mri/segmentation/status/{request_id}/
```

### 3. 성능 최적화

- **GPU 사용**: 추론 시간 20초 vs 15분 (CPU)
- **폴링 간격 조정**: `POLL_INTERVAL=10` (더 빠른 감지, 더 많은 CPU 사용)
- **병렬 처리**: 여러 워커 실행 (고급)

---

## 🎉 완료!

이제 프론트엔드에서 "AI 분석" 버튼을 누르면:

1. ✅ 자동으로 연구실 컴퓨터 워커에 요청 생성
2. ✅ 워커가 자동으로 처리
3. ✅ 결과가 Orthanc에 업로드
4. ✅ 프론트엔드에 결과 표시

**연구실 컴퓨터 워커만 실행 중이면 자동으로 작동합니다!** 🚀

---

**작성일**: 2026년 1월
**버전**: 1.0.0
