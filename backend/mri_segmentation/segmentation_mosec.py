#!/usr/bin/env python3
"""
Mosec ê¸°ë°˜ MRI ì„¸ê·¸ë©˜í…Œì´ì…˜ ì„œë²„
Sliding Window Inferenceë¥¼ ì‚¬ìš©í•˜ì—¬ 128Ã—128Ã—128 ëª¨ë¸ë¡œ ì „ì²´ ë³¼ë¥¨ ì²˜ë¦¬
- ëª¨ë¸ í•™ìŠµ í¬ê¸°: [4, 128, 128, 128] (4 channels, 128 depth, 128 height, 128 width)
- ì‹¤ì œ ì²˜ë¦¬: [4, D, H, W] (DëŠ” ì „ì²´ ìŠ¬ë¼ì´ìŠ¤ ìˆ˜, ì˜ˆ: 134)
- Sliding Window: roi_size=(128, 128, 128), overlap=0.25
"""
import os
import io
import base64
import logging
import numpy as np
import torch
from monai.inferers import sliding_window_inference
from monai.transforms import (
    Compose, LoadImaged, EnsureChannelFirstd, Orientationd,
    Spacingd, NormalizeIntensityd, EnsureTyped
)
import pydicom
from PIL import Image
from scipy import ndimage
from scipy.ndimage import binary_opening, binary_closing, gaussian_filter
from datetime import datetime
from pydicom.uid import generate_uid
from pydicom.dataset import Dataset, FileDataset
import re
import requests
import tempfile
from pathlib import Path
import SimpleITK as sitk
import nibabel as nib

from mosec import Server, Worker, ValidationError
from monai.networks.nets import SwinUNETR

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜
MODEL_PATH = '/home/shrjsdn908/models/mri_models/Phase1_Segmentation_best.pth'
ORTHANC_URL = 'http://localhost:8042'
ORTHANC_USER = 'admin'
ORTHANC_PASSWORD = 'admin123'


def dicom_to_numpy(dicom_bytes):
    """DICOM ë°”ì´íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜ (ì¡°ì› ì½”ë“œì™€ ë™ì¼í•œ ì •ê·œí™”)"""
    dicom = pydicom.dcmread(io.BytesIO(dicom_bytes))
    pixel_array = dicom.pixel_array.astype(np.float32)
    
    # ì¡°ì› ì½”ë“œì™€ ë™ì¼í•œ ì •ê·œí™”: nonzero=True, channel_wise=True (Z-score normalization)
    # nonzero=True: 0ì´ ì•„ë‹Œ ê°’ì— ëŒ€í•´ì„œë§Œ ì •ê·œí™”
    non_zero_mask = pixel_array != 0
    if non_zero_mask.any():
        mean = pixel_array[non_zero_mask].mean()
        std = pixel_array[non_zero_mask].std()
        if std > 0:
            pixel_array[non_zero_mask] = (pixel_array[non_zero_mask] - mean) / std
    
    return pixel_array, dicom



def convert_dicom_series_to_nifti(dicom_dir, output_path=None):
    """DICOM ì‹œë¦¬ì¦ˆë¥¼ NIfTIë¡œ ë³€í™˜ (ì¡°ì› ì½”ë“œì™€ ë™ì¼)"""
    try:
        dicom_dir = Path(dicom_dir)
        
        # Read DICOM series using SimpleITK
        reader = sitk.ImageSeriesReader()
        dicom_files = reader.GetGDCMSeriesFileNames(str(dicom_dir))
        
        if len(dicom_files) == 0:
            raise ValueError(f"No DICOM files found in {dicom_dir}")
        
        reader.SetFileNames(dicom_files)
        image = reader.Execute()
        
        # Convert to numpy array
        array = sitk.GetArrayFromImage(image)
        
        # Get affine matrix
        spacing = image.GetSpacing()
        origin = image.GetOrigin()
        direction = np.array(image.GetDirection()).reshape(3, 3)
        
        # Create affine
        affine = np.eye(4)
        affine[:3, :3] = direction * np.array(spacing)
        affine[:3, 3] = origin
        
        # Transpose array to match NIfTI convention (SimpleITK uses different ordering)
        array = np.transpose(array, (2, 1, 0))
        
        # Create NIfTI image
        nifti_img = nib.Nifti1Image(array, affine)
        
        # Save to file
        if output_path is None:
            temp_dir = tempfile.mkdtemp()
            output_path = os.path.join(temp_dir, "converted.nii.gz")
        
        nib.save(nifti_img, output_path)
        logger.info(f"âœ… DICOM â†’ NIfTI ë³€í™˜ ì™„ë£Œ: {output_path}")
        logger.info(f"  Shape: {array.shape}, Spacing: {spacing}")
        
        return output_path
        
    except Exception as e:
        logger.error(f"âŒ DICOM â†’ NIfTI ë³€í™˜ ì‹¤íŒ¨: {e}")
        raise

def create_4d_input_from_sequences(sequences_3d, target_spatial=None, target_depth=None):
    """4ê°œ ì‹œí€€ìŠ¤ì˜ 3D ë³¼ë¥¨ì„ [4, D, H, W]ë¡œ ë³€í™˜ (ì›ë³¸ í¬ê¸° ìœ ì§€ ë˜ëŠ” ë¦¬ì‚¬ì´ì¦ˆ)
    
    Args:
        sequences_3d: list of 4 numpy arrays, ê°ê° [D, H, W] í˜•íƒœ
        target_spatial: ê³µê°„ í¬ê¸° (Noneì´ë©´ ì›ë³¸ ìœ ì§€)
        target_depth: ê¹Šì´ í¬ê¸° (Noneì´ë©´ ì›ë³¸ ìœ ì§€)
    
    Returns:
        volume_4d: [4, D, H, W] numpy array
    """
    from scipy.ndimage import zoom
    
    if target_spatial is None or target_depth is None:
        # ì¡°ì› ì½”ë“œì™€ ë™ì¼í•˜ê²Œ 1.5mm spacingìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§
        from scipy.ndimage import zoom
        target_spacing = 1.5  # ì¡°ì› ì½”ë“œì™€ ë™ì¼ (config.SPACING = (1.5, 1.5, 1.5))
        
        resized_sequences = []
        for seq_3d in sequences_3d:
            d, h, w = seq_3d.shape
            # DICOMì—ì„œ spacing ì •ë³´ë¥¼ ê°€ì ¸ì™€ì•¼ í•˜ì§€ë§Œ, ì¼ë‹¨ 1.5mmë¡œ ê°€ì •
            # ì‹¤ì œë¡œëŠ” DICOM íƒœê·¸ì—ì„œ PixelSpacing, SliceThicknessë¥¼ ê°€ì ¸ì™€ì•¼ í•¨
            # í˜„ì¬ëŠ” ì›ë³¸ í¬ê¸° ìœ ì§€í•˜ë˜, ë‚˜ì¤‘ì— spacing ì •ë³´ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ êµ¬ì¡° ìœ ì§€
            resized_sequences.append(seq_3d)
        
        volume_4d = np.stack(resized_sequences, axis=0)
        logger.info(f"âœ… 3D ë³¼ë¥¨ ìƒì„± ì™„ë£Œ (ì›ë³¸ í¬ê¸° ìœ ì§€, spacing ë¦¬ìƒ˜í”Œë§ì€ ì¶”í›„ ì¶”ê°€): {volume_4d.shape} (4 channels, {volume_4d.shape[1]} depth, {volume_4d.shape[2]}Ã—{volume_4d.shape[3]})")
        return volume_4d
    
    # ë¦¬ì‚¬ì´ì¦ˆ ëª¨ë“œ
    resized_sequences = []
    for seq_3d in sequences_3d:
        d, h, w = seq_3d.shape
        zoom_factors = (target_depth / d, target_spatial / h, target_spatial / w)
        resized = zoom(seq_3d, zoom_factors, order=1)
        resized_sequences.append(resized)
    
    # [4, D, H, W]
    volume_4d = np.stack(resized_sequences, axis=0)
    
    logger.info(f"âœ… 3D ë³¼ë¥¨ ìƒì„± ì™„ë£Œ (ë¦¬ì‚¬ì´ì¦ˆ): {volume_4d.shape} (4 channels, {target_depth} depth, {target_spatial}Ã—{target_spatial})")
    return volume_4d


def create_mock_4d_input(slice_2d):
    """ë‹¨ì¼ 2D ìŠ¬ë¼ì´ìŠ¤ë¥¼ 4D MRI ì…ë ¥ìœ¼ë¡œ ë³€í™˜ (fallback)"""
    mock_3d = np.stack([slice_2d] * 128, axis=0)  # [128, H, W]
    return create_4d_input_from_sequences([mock_3d] * 4)


def postprocess_mask(mask, smooth_boundary=False):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬ (ì¡°ì› ì½”ë“œì™€ ë™ì¼í•˜ê²Œ)
    
    Args:
        mask: ì…ë ¥ ë§ˆìŠ¤í¬ (2D numpy array, ì´ë¯¸ ì´ì§„í™”ë¨)
        smooth_boundary: ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ì¡°ì› ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€)
    
    Returns:
        mask_cleaned: í›„ì²˜ë¦¬ëœ ë§ˆìŠ¤í¬
    """
    # ì¡°ì› ì½”ë“œì™€ ë™ì¼í•œ ìˆœì„œ: Keep largest component â†’ Fill holes
    # 1. ê°€ì¥ í° ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸ë§Œ ìœ ì§€
    labeled, num_features = ndimage.label(mask)
    if num_features > 0:
        sizes = ndimage.sum(mask, labeled, range(1, num_features + 1))
        largest_component = np.argmax(sizes) + 1
        binary_mask = (labeled == largest_component).astype(np.uint8)
    else:
        binary_mask = mask.astype(np.uint8)
    
    # 2. Fill holes (ì¡°ì› ì½”ë“œì™€ ë™ì¼)
    binary_mask = ndimage.binary_fill_holes(binary_mask).astype(np.uint8)
    
    return binary_mask


def create_dicom_seg(original_dicom, mask_array, seg_series_uid, instance_number, original_series_id):
    """ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ë¥¼ DICOM SEG íŒŒì¼ë¡œ ë³€í™˜"""
    file_meta = Dataset()
    file_meta.TransferSyntaxUID = pydicom.uid.ExplicitVRLittleEndian
    file_meta.MediaStorageSOPClassUID = '1.2.840.10008.5.1.4.1.1.7'
    file_meta.MediaStorageSOPInstanceUID = generate_uid()
    file_meta.ImplementationClassUID = generate_uid()
    
    ds = FileDataset(None, {}, file_meta=file_meta, preamble=b"\0" * 128)
    
    # í™˜ì ì •ë³´
    ds.PatientName = getattr(original_dicom, 'PatientName', 'Anonymous')
    ds.PatientID = getattr(original_dicom, 'PatientID', 'Unknown')
    ds.PatientBirthDate = getattr(original_dicom, 'PatientBirthDate', '')
    ds.PatientSex = getattr(original_dicom, 'PatientSex', '')
    # í•œê¸€ ì§€ì›ì„ ìœ„í•œ ë¬¸ìì…‹ ì„¤ì •
    ds.SpecificCharacterSet = 'ISO_IR 192'  # UTF-8
    
    # ìŠ¤í„°ë”” ì •ë³´
    ds.StudyInstanceUID = safe_get_uid(original_dicom, 'StudyInstanceUID', generate_uid)
    ds.StudyDate = getattr(original_dicom, 'StudyDate', datetime.now().strftime('%Y%m%d'))
    ds.StudyTime = getattr(original_dicom, 'StudyTime', datetime.now().strftime('%H%M%S'))
    ds.StudyID = getattr(original_dicom, 'StudyID', '')
    ds.AccessionNumber = getattr(original_dicom, 'AccessionNumber', '')
    
    # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œë¦¬ì¦ˆ ì •ë³´
    ds.SeriesInstanceUID = seg_series_uid
    ds.SeriesNumber = '9999'
    # SeriesDescription: 64ì ì œí•œ (VR LO ìµœëŒ€ ê¸¸ì´)
    series_desc = f'AI Tumor Segmentation (Series: {original_series_id[:30] if len(str(original_series_id)) > 30 else original_series_id})'[:64]
    ds.SeriesDescription = series_desc
    ds.Modality = 'SEG'
    
    # SOP Instance ì •ë³´
    ds.SOPClassUID = '1.2.840.10008.5.1.4.1.1.7'
    ds.SOPInstanceUID = file_meta.MediaStorageSOPInstanceUID
    ds.InstanceNumber = str(instance_number)
    
    # í”½ì…€ ë°ì´í„° ì •ë³´
    ds.Rows = mask_array.shape[0]
    ds.Columns = mask_array.shape[1]
    ds.SamplesPerPixel = 1
    ds.PhotometricInterpretation = 'MONOCHROME2'
    ds.BitsAllocated = 8
    ds.BitsStored = 8
    ds.HighBit = 7
    ds.PixelRepresentation = 0
    
    pixel_data = (mask_array * 255).astype(np.uint8)
    ds.PixelData = pixel_data.tobytes()
    
    # ê¸°íƒ€ ì •ë³´
    ds.ContentDate = datetime.now().strftime('%Y%m%d')
    ds.ContentTime = datetime.now().strftime('%H%M%S')
    ds.ImageType = ['DERIVED', 'SECONDARY', 'AI_SEGMENTATION']
    
    logger.info(f"âœ… DICOM SEG íŒŒì¼ ìƒì„± ì™„ë£Œ: {ds.SOPInstanceUID} (Series: {seg_series_uid}, Instance: {instance_number})")
    return ds



def is_valid_uid(uid):
    """DICOM UID í˜•ì‹ ê²€ì¦: ì (.)ìœ¼ë¡œ êµ¬ë¶„ëœ ìˆ«ìë§Œ í—ˆìš©"""
    if not uid or not isinstance(uid, str):
        return False
    pattern = r'^[0-9]+(\.[0-9]+)*$'
    return bool(re.match(pattern, str(uid)))

def safe_get_uid(dicom_obj, attr_name, default_func):
    """ì›ë³¸ DICOMì—ì„œ UIDë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° (ê²€ì¦ í›„)"""
    uid = getattr(dicom_obj, attr_name, None)
    if uid and is_valid_uid(str(uid)):
        return str(uid)
    logger.warning(f"âš ï¸ ì˜ëª»ëœ {attr_name} í˜•ì‹: {uid}, ìƒˆ UID ìƒì„±")
    return default_func()

def create_dicom_seg_multiframe(original_dicom, mask_array_3d, seg_series_uid, start_instance_number, original_series_id):
    """
    3D ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ë¥¼ Multi-frame DICOM SEGë¡œ ë³€í™˜
    
    Args:
        original_dicom: ì›ë³¸ DICOM íŒŒì¼
        mask_array_3d: (96, H, W) í˜•íƒœì˜ 3D ë§ˆìŠ¤í¬
        seg_series_uid: ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œë¦¬ì¦ˆ UID
        start_instance_number: ì‹œì‘ Instance ë²ˆí˜¸
        original_series_id: ì›ë³¸ ì‹œë¦¬ì¦ˆ ID
    """
    num_frames = mask_array_3d.shape[0]
    
    file_meta = Dataset()
    file_meta.TransferSyntaxUID = pydicom.uid.ExplicitVRLittleEndian
    file_meta.MediaStorageSOPClassUID = '1.2.840.10008.5.1.4.1.1.66.4'  # Segmentation Storage
    file_meta.MediaStorageSOPInstanceUID = generate_uid()
    file_meta.ImplementationClassUID = generate_uid()
    
    ds = FileDataset(None, {}, file_meta=file_meta, preamble=b"\0" * 128)
    
    # í™˜ì ì •ë³´
    ds.PatientName = getattr(original_dicom, 'PatientName', 'Anonymous')
    ds.PatientID = getattr(original_dicom, 'PatientID', 'Unknown')
    ds.PatientBirthDate = getattr(original_dicom, 'PatientBirthDate', '')
    ds.PatientSex = getattr(original_dicom, 'PatientSex', '')
    # í•œê¸€ ì§€ì›ì„ ìœ„í•œ ë¬¸ìì…‹ ì„¤ì •
    ds.SpecificCharacterSet = 'ISO_IR 192'  # UTF-8
    
    # ìŠ¤í„°ë”” ì •ë³´
    ds.StudyInstanceUID = safe_get_uid(original_dicom, 'StudyInstanceUID', generate_uid)
    ds.StudyDate = getattr(original_dicom, 'StudyDate', datetime.now().strftime('%Y%m%d'))
    ds.StudyTime = getattr(original_dicom, 'StudyTime', datetime.now().strftime('%H%M%S'))
    ds.StudyID = getattr(original_dicom, 'StudyID', '')
    ds.AccessionNumber = getattr(original_dicom, 'AccessionNumber', '')
    
    # ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œë¦¬ì¦ˆ ì •ë³´
    ds.SeriesInstanceUID = seg_series_uid
    ds.SeriesNumber = '9999'
    # SeriesDescription: 64ì ì œí•œ (VR LO ìµœëŒ€ ê¸¸ì´)
    series_desc = f'AI Tumor Segmentation (Series: {original_series_id[:30] if len(str(original_series_id)) > 30 else original_series_id})'[:64]
    ds.SeriesDescription = series_desc
    ds.Modality = 'SEG'
    
    # SOP Instance ì •ë³´
    ds.SOPClassUID = '1.2.840.10008.5.1.4.1.1.66.4'  # Segmentation Storage
    ds.SOPInstanceUID = file_meta.MediaStorageSOPInstanceUID
    ds.InstanceNumber = str(start_instance_number)
    
    # Multi-frame í”½ì…€ ë°ì´í„° ì •ë³´
    ds.NumberOfFrames = num_frames
    ds.Rows = mask_array_3d.shape[1]
    ds.Columns = mask_array_3d.shape[2]
    ds.SamplesPerPixel = 1
    ds.PhotometricInterpretation = 'MONOCHROME2'
    ds.BitsAllocated = 8
    ds.BitsStored = 8
    ds.HighBit = 7
    ds.PixelRepresentation = 0
    
    # 96ê°œ í”„ë ˆì„ì„ í•˜ë‚˜ì˜ PixelDataë¡œ ê²°í•©
    pixel_data_list = []
    for i in range(num_frames):
        frame_data = (mask_array_3d[i] * 255).astype(np.uint8)
        pixel_data_list.append(frame_data.tobytes())
    
    ds.PixelData = b''.join(pixel_data_list)
    
    # DICOM ì¸ì½”ë”© ì„¤ì • (í•„ìˆ˜!)
    ds.is_little_endian = True
    ds.is_implicit_VR = False
    
    # ê¸°íƒ€ ì •ë³´
    ds.ContentDate = datetime.now().strftime('%Y%m%d')
    ds.ContentTime = datetime.now().strftime('%H%M%S')
    ds.ImageType = ['DERIVED', 'SECONDARY', 'AI_SEGMENTATION']
    
    logger.info(f"âœ… DICOM SEG íŒŒì¼ ìƒì„± ì™„ë£Œ: {ds.SOPInstanceUID} (Series: {seg_series_uid}, Instance: {start_instance_number}, Frames: {num_frames})")
    return ds


def upload_to_orthanc(dicom_dataset, orthanc_url=None, orthanc_auth=None):
    """DICOM íŒŒì¼ì„ Orthancì— ì—…ë¡œë“œ"""
    with tempfile.NamedTemporaryFile(suffix='.dcm', delete=False) as tmp:
        try:
            dicom_dataset.save_as(tmp.name)
            with open(tmp.name, 'rb') as f:
                dicom_bytes = f.read()
            
            response = requests.post(
                f"{orthanc_url or ORTHANC_URL}/instances",
                auth=orthanc_auth or (ORTHANC_USER, ORTHANC_PASSWORD),
                headers={'Content-Type': 'application/dicom'},
                data=dicom_bytes,
                timeout=30
            )
            
            # ì—ëŸ¬ ì‘ë‹µ ìì„¸íˆ ë¡œê¹…
            if response.status_code != 200:
                logger.error(f"âŒ Orthanc ì—…ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
                try:
                    error_detail = response.json()
                    logger.error(f"   ì—ëŸ¬ ìƒì„¸: {error_detail}")
                except:
                    logger.error(f"   ì—ëŸ¬ ì‘ë‹µ: {response.text[:500]}")
                response.raise_for_status()
            
            result = response.json()
            instance_id = result.get('ID')
            logger.info(f"âœ… Orthanc ì—…ë¡œë“œ ì™„ë£Œ: Instance ID = {instance_id}")
            return instance_id
        except Exception as e:
            logger.error(f"âš ï¸ Orthanc ì €ì¥ ì‹¤íŒ¨: {type(e).__name__}: {e}")
            raise
        finally:
            if os.path.exists(tmp.name):
                os.unlink(tmp.name)


class SegmentationWorker(Worker):
    def __init__(self):
        super().__init__()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.pipeline = None  # MAMA_MIA_JO_WON_PKG pipeline
        logger.info(f"ğŸ’» Device: {self.device}")
    
    def deserialize(self, data: bytes) -> dict:
        """ìš”ì²­ ë°ì´í„° ì—­ì§ë ¬í™” (Orthanc API ë°©ì‹)"""
        try:
            import json
            import requests
            import base64
            
            json_data = json.loads(data.decode('utf-8'))
            
            logger.info(f"ğŸ“¥ ìˆ˜ì‹ í•œ ë°ì´í„° í‚¤: {list(json_data.keys())}")
            
            # ê¸°ë³¸ Orthanc ì„¤ì • (orthanc_instance_idsê°€ ì—†ì„ ê²½ìš° ëŒ€ë¹„)
            orthanc_url = ORTHANC_URL
            orthanc_auth = (ORTHANC_USER, ORTHANC_PASSWORD)
            # Orthanc Instance ID ëª©ë¡ì´ ìˆìœ¼ë©´ Orthanc APIë¡œ ë‹¤ìš´ë¡œë“œ
            if "orthanc_instance_ids" in json_data:
                orthanc_url = json_data["orthanc_url"]
                orthanc_auth = tuple(json_data["orthanc_auth"])
                
                total_slices = len(json_data['orthanc_instance_ids'][0])
                logger.info(f"ğŸ“¥ Orthancì—ì„œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘: {orthanc_url}")
                logger.info(f"ğŸ“Š ì´ {len(json_data['orthanc_instance_ids'])}ê°œ ì‹œí€€ìŠ¤, ê° {total_slices}ê°œ ìŠ¬ë¼ì´ìŠ¤ (ì „ì²´ ì²˜ë¦¬)")
                
                sequences_3d = []
                for seq_idx, seq_instances in enumerate(json_data["orthanc_instance_ids"]):
                    slices_data = []
                    for slice_idx, instance_id in enumerate(seq_instances):
                        # Orthanc APIë¡œ DICOM íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                        response = requests.get(
                            f"{orthanc_url}/instances/{instance_id}/file",
                            auth=orthanc_auth,
                            timeout=30
                        )
                        response.raise_for_status()
                        
                        # Base64 ì¸ì½”ë”©
                        slices_data.append(base64.b64encode(response.content).decode('utf-8'))
                        
                        if (slice_idx + 1) % 20 == 0:
                            logger.info(f"  ì‹œí€€ìŠ¤ {seq_idx+1}: {slice_idx+1}/{len(seq_instances)} ìŠ¬ë¼ì´ìŠ¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                    
                    sequences_3d.append(slices_data)
                    logger.info(f"âœ… ì‹œí€€ìŠ¤ {seq_idx+1}/4 ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(slices_data)}ê°œ ìŠ¬ë¼ì´ìŠ¤")
                
                return {
                    "sequences_3d": sequences_3d,
                    "seg_series_uid": json_data.get("seg_series_uid"),
                    "original_series_id": json_data.get("original_series_id"),
                    "start_instance_number": json_data.get("start_instance_number", 1),
                    "total_slices": json_data.get("total_slices", total_slices),  # ì „ì²´ ìŠ¬ë¼ì´ìŠ¤ ìˆ˜ ì „ë‹¬
                    "orthanc_url": orthanc_url,
                    "orthanc_auth": orthanc_auth
                }
            
            # ê¸°ì¡´ ë°©ì‹ (sequences_3dê°€ ì§ì ‘ í¬í•¨ëœ ê²½ìš°)
            if "sequences_3d" in json_data or "sequences" in json_data:
                logger.info("ğŸ“¥ 4-channel JSON ì…ë ¥ ê°ì§€")
                return json_data
                
        except Exception as e:
            logger.error(f"ì—­ì§ë ¬í™” ì‹¤íŒ¨: {e}", exc_info=True)
            return {"error": str(e)}
        
        logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ë°ì´í„° í˜•ì‹. ë°›ì€ í‚¤: {list(json_data.keys()) if 'json_data' in locals() else 'JSON íŒŒì‹± ì‹¤íŒ¨'}")
        return {}

    def forward(self, data: dict) -> dict:
        """ì„¸ê·¸ë©˜í…Œì´ì…˜ ì¶”ë¡ """
        try:
            # ëª¨ë¸ ë¡œë“œ
            if self.model is None:
                logger.info(f"ğŸ”„ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_PATH}")
                
                self.model = SwinUNETR(
                    spatial_dims=3,
                    in_channels=4,
                    out_channels=1,
                    feature_size=24,  # 128Ã—128Ã—128 ëª¨ë¸ê³¼ ë™ì¼í•œ feature_size
                    use_checkpoint=False,
                )
                
                checkpoint = torch.load(MODEL_PATH, map_location=self.device, weights_only=False)
                
                if 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                else:
                    state_dict = checkpoint
                
                # í‚¤ ì´ë¦„ ë³€í™˜
                new_state_dict = {}
                for key, value in state_dict.items():
                    if 'lora_A' in key or 'lora_B' in key:
                        continue
                    new_key = key.replace('model.base_model.model.', '')
                    new_key = new_key.replace('.base_layer', '')
                    new_state_dict[new_key] = value
                
                self.model.load_state_dict(new_state_dict, strict=False)
                self.model = self.model.to(self.device)
                self.model.eval()
                logger.info("âœ… ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            # ì¡°ì› ì½”ë“œì™€ ë™ì¼í•˜ê²Œ: DICOM â†’ NIfTI ë³€í™˜ â†’ MONAI transforms
            original_dicom = None  # ì´ˆê¸°í™” (í›„ì²˜ë¦¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡)
            if "sequences_3d" in data and len(data["sequences_3d"]) == 4:
                # ì²« ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ ì²« ë²ˆì§¸ ìŠ¬ë¼ì´ìŠ¤ì—ì„œ ì›ë³¸ DICOM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                if len(data["sequences_3d"]) > 0 and len(data["sequences_3d"][0]) > 0:
                    first_slice_b64 = data["sequences_3d"][0][0]
                    first_slice_bytes = base64.b64decode(first_slice_b64)
                    original_dicom = pydicom.dcmread(io.BytesIO(first_slice_bytes))
                    # ì›ë³¸ spacing ì •ë³´ ì¶”ì¶œ

                    original_spacing = None

                    if hasattr(original_dicom, 'PixelSpacing') and hasattr(original_dicom, 'SliceThickness'):

                        pixel_spacing = list(original_dicom.PixelSpacing)  # [row, col] in mm

                        slice_thickness = float(original_dicom.SliceThickness)  # in mm

                        original_spacing = [pixel_spacing[0], pixel_spacing[1], slice_thickness]  # [x, y, z] in mm

                        logger.info(f"ğŸ“ ì›ë³¸ spacing ì¶”ì¶œ: {original_spacing} mm")

                    elif hasattr(original_dicom, 'PixelSpacing'):

                        pixel_spacing = list(original_dicom.PixelSpacing)

                        original_spacing = [pixel_spacing[0], pixel_spacing[1], 1.0]  # ê¸°ë³¸ê°’

                        logger.info(f"ğŸ“ ì›ë³¸ spacing ì¶”ì¶œ (SliceThickness ì—†ìŒ): {original_spacing} mm")

                    # spacing ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                    if original_spacing is None:
                        logger.warning("âš ï¸ ì›ë³¸ spacing ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ [0.5, 0.5, 0.5] ì‚¬ìš©")
                        original_spacing = [0.5, 0.5, 0.5]  # ê¸°ë³¸ê°’ (ì¼ë°˜ì ì¸ MRI spacing)
                    
                    logger.info(f"âœ… ì›ë³¸ DICOM ë©”íƒ€ë°ì´í„° ì¶”ì¶œ: PatientID={getattr(original_dicom, 'PatientID', 'Unknown')}, PatientName={getattr(original_dicom, 'PatientName', 'Unknown')}")
                else:
                    logger.warning("âš ï¸ ì²« ë²ˆì§¸ ìŠ¬ë¼ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ original_dicomì„ Noneìœ¼ë¡œ ì„¤ì •")
                
                # 4-channel 3D DCE-MRI ëª¨ë“œ (ì¡°ì› ì½”ë“œì™€ ë™ì¼í•œ íŒŒì´í”„ë¼ì¸)
                total_slices = data.get("total_slices", len(data["sequences_3d"][0]))
                logger.info(f"ğŸ“Š 4-channel 3D DCE-MRI ì…ë ¥ ê°ì§€ ({total_slices} slices per sequence) - ì¡°ì› ì½”ë“œì™€ ë™ì¼í•œ ì „ì²˜ë¦¬")
                
                # ì›ë³¸ DICOM í¬ê¸° ì •ë³´ ì €ì¥ (spacing ë³µì›ìš©)
                if original_dicom is not None:
                    original_size = [
                        total_slices,  # D (ìŠ¬ë¼ì´ìŠ¤ ìˆ˜)
                        int(original_dicom.Rows) if hasattr(original_dicom, "Rows") else 256,  # H
                        int(original_dicom.Columns) if hasattr(original_dicom, "Columns") else 256  # W
                    ]
                    logger.info(f"ğŸ“ ì›ë³¸ DICOM í¬ê¸° ì €ì¥: {original_size} (ìŠ¬ë¼ì´ìŠ¤ ìˆ˜, H, W)")

                # 1. DICOM ìŠ¬ë¼ì´ìŠ¤ë“¤ì„ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
                temp_base_dir = tempfile.mkdtemp()
                nifti_files = []
                
                try:
                    for seq_idx, seq_slices_b64 in enumerate(data["sequences_3d"]):
                        # ê° ì‹œí€€ìŠ¤ë³„ ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
                        seq_dir = os.path.join(temp_base_dir, f"sequence_{seq_idx}")
                        os.makedirs(seq_dir, exist_ok=True)
                        
                        # DICOM ìŠ¬ë¼ì´ìŠ¤ë“¤ì„ íŒŒì¼ë¡œ ì €ì¥
                        for slice_idx, slice_b64 in enumerate(seq_slices_b64):
                            slice_bytes = base64.b64decode(slice_b64)
                            dicom_file = os.path.join(seq_dir, f"slice_{slice_idx:04d}.dcm")
                            with open(dicom_file, "wb") as f:
                                f.write(slice_bytes)
                        
                        # DICOM ì‹œë¦¬ì¦ˆë¥¼ NIfTIë¡œ ë³€í™˜ (ì¡°ì› ì½”ë“œì™€ ë™ì¼)
                        nifti_file = convert_dicom_series_to_nifti(seq_dir)
                        nifti_files.append(nifti_file)
                        logger.info(f"âœ… ì‹œí€€ìŠ¤ {seq_idx+1}/4: DICOM â†’ NIfTI ë³€í™˜ ì™„ë£Œ")
                    
                    # 2. MONAI transforms ì ìš© (ì¡°ì› ì½”ë“œì™€ ì™„ì „íˆ ë™ì¼)
                    transforms = Compose([
                        LoadImaged(keys=["image"], image_only=False),  # NIfTI ë¡œë“œ
                        EnsureChannelFirstd(keys=["image"]),  # ì±„ë„ ìˆœì„œ í™•ì¸
                        Orientationd(keys=["image"], axcodes="RAS"),  # âœ… RAS ë³€í™˜
                        Spacingd(keys=["image"], pixdim=(1.5, 1.5, 1.5), mode="bilinear"),  # âœ… 1.5mm ë¦¬ìƒ˜í”Œë§
                        NormalizeIntensityd(keys=["image"], nonzero=True, channel_wise=True),  # âœ… ì •ê·œí™”
                        EnsureTyped(keys=["image"], dtype=torch.float32)  # Tensor ë³€í™˜
                    ])
                    
                    # 3. ê° ì‹œí€€ìŠ¤ë¥¼ ì „ì²˜ë¦¬í•˜ê³  4ì±„ë„ë¡œ í•©ì¹˜ê¸°
                    preprocessed_sequences = []
                    for nifti_file in nifti_files:
                        data_dict = {"image": nifti_file}
                        preprocessed = transforms(data_dict)
                        seq_tensor = preprocessed["image"]  # [C, H, W, D] í˜•íƒœ (C=1)
                        preprocessed_sequences.append(seq_tensor)
                    
                    # 4. 4ì±„ë„ë¡œ í•©ì¹˜ê¸°: [4, H, W, D]
                    volume_4d = torch.cat(preprocessed_sequences, dim=0)  # [4, H, W, D]
                    logger.info(f"âœ… ì¡°ì› ì½”ë“œì™€ ë™ì¼í•œ ì „ì²˜ë¦¬ ì™„ë£Œ: {volume_4d.shape}")
                    
                    # 5. Batch dimension ì¶”ê°€: [1, 4, H, W, D]
                    input_tensor = volume_4d.unsqueeze(0).to(self.device)
                    
                    # MAMA_MIA_JO_WON_PKG ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
                    if self.pipeline is None:
                        import sys
                        sys.path.insert(0, "/home/shrjsdn908/MAMA_MIA_JO_WON_PKG/src")
                        from inference_pipeline import SegmentationInferencePipeline
                        
                        logger.info(f"ğŸ”„ MAMA_MIA_JO_WON_PKG ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ë¡œë”© ì¤‘: {MODEL_PATH}")
                        self.pipeline = SegmentationInferencePipeline(
                            model_path=MODEL_PATH,
                            device="cpu",
                            threshold=0.5
                        )
                        logger.info("âœ… MAMA_MIA_JO_WON_PKG íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ")
                    
                    # MAMA_MIA_JO_WON_PKG pipelineìœ¼ë¡œ ì¶”ë¡ 
                    logger.info(f"ğŸš€ MAMA_MIA_JO_WON_PKG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì¶”ë¡  ì‹œì‘...")
                    result = self.pipeline.predict(
                        image_path=temp_base_dir,
                        output_format="dicom",
                        return_probabilities=False
                    )
                    
                    # ê²°ê³¼ ì¶”ì¶œ
                    tumor_detected = result.get("tumor_detected", False)
                    tumor_volume_voxels = result.get("tumor_volume_voxels", 0)
                    segmentation_mask = result.get("segmentation")
                    
                    logger.info(f"âœ… MAMA_MIA_JO_WON_PKG ì¶”ë¡  ì™„ë£Œ: tumor_detected={tumor_detected}, volume={tumor_volume_voxels}")
                    
                    # segmentation_maskë¥¼ mask_resized_3dë¡œ ë³€í™˜
                    if segmentation_mask is not None:
                        # segmentation_maskëŠ” numpy array [D, H, W] í˜•íƒœ
                        if segmentation_mask.ndim == 2:
                            segmentation_mask = segmentation_mask[np.newaxis, ...]
                        elif segmentation_mask.ndim == 4:
                            segmentation_mask = segmentation_mask.squeeze(0).squeeze(0)
                        elif segmentation_mask.ndim == 3 and segmentation_mask.shape[0] == 1:
                            segmentation_mask = segmentation_mask.squeeze(0)
                        
                        # mask_resized_3d ì´ˆê¸°í™” (ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜)
                        mask_resized_3d = segmentation_mask.astype(np.uint8)
                        logger.info(f"âœ… MAMA_MIA_JO_WON_PKG ê²°ê³¼ ë³€í™˜ ì™„ë£Œ: {mask_resized_3d.shape}")
                        
                        # ì›ë³¸ í¬ê¸° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                        if original_dicom is not None:
                            h = getattr(original_dicom, "Rows", mask_resized_3d.shape[1])
                            w = getattr(original_dicom, "Columns", mask_resized_3d.shape[2])
                        else:
                            h, w = mask_resized_3d.shape[1], mask_resized_3d.shape[2]
                    else:
                        # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë°°ì—´ë¡œ ì´ˆê¸°í™” (ì—ëŸ¬ ë°©ì§€)
                        mask_resized_3d = np.zeros((1, 256, 256), dtype=np.uint8)
                        raise ValueError("MAMA_MIA_JO_WON_PKG íŒŒì´í”„ë¼ì¸ì—ì„œ segmentation_maskë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    
                    
                finally:
                    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                    import shutil
                    try:
                        shutil.rmtree(temp_base_dir)
                    except:
                        pass
            elif "dicom_data" in data:
                # ë‹¨ì¼ ì´ë¯¸ì§€ ëª¨ë“œ (JSON with base64)
                logger.info("ğŸ“Š ë‹¨ì¼ ì´ë¯¸ì§€ ì…ë ¥ ê°ì§€ (JSON)")
                dicom_bytes = base64.b64decode(data["dicom_data"])
                slice_2d, original_dicom = dicom_to_numpy(dicom_bytes)
                volume_4d = create_mock_4d_input(slice_2d)
            elif "dicom_bytes" in data:
                # ë‹¨ì¼ ì´ë¯¸ì§€ ëª¨ë“œ (raw bytes - fallback)
                logger.info("ğŸ“Š ë‹¨ì¼ ì´ë¯¸ì§€ ì…ë ¥ ê°ì§€ (raw bytes)")
                dicom_bytes = data["dicom_bytes"]
                slice_2d, original_dicom = dicom_to_numpy(dicom_bytes)
                volume_4d = create_mock_4d_input(slice_2d)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹ì…ë‹ˆë‹¤. Keys: {list(data.keys())}")
            
            # input_tensorê°€ ì•„ì§ ì •ì˜ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìƒì„± (ë‹¨ì¼ ì´ë¯¸ì§€ ëª¨ë“œ)
            if "input_tensor" not in locals():
                input_tensor = torch.from_numpy(volume_4d).unsqueeze(0).float().to(self.device)
            
            # Sliding Window Inferenceë¡œ ì „ì²´ ë³¼ë¥¨ ì²˜ë¦¬
            # ëª¨ë¸ì€ 128Ã—128Ã—128 íŒ¨ì¹˜ë¡œ í•™ìŠµë˜ì—ˆì§€ë§Œ, sliding windowë¡œ ë” í° ë³¼ë¥¨ ì²˜ë¦¬ ê°€ëŠ¥
                    # ê¸°ì¡´ ì¶”ë¡  ì½”ë“œëŠ” MAMA_MIA_JO_WON_PKGë¡œ ëŒ€ì²´ë¨
                    # with torch.no_grad():
                    # logger.info(f"ğŸ”„ Sliding Window Inference ì‹œì‘: roi_size=(128, 128, 128), overlap=0.25, mode=gaussian")
                    # output = sliding_window_inference(
                    # inputs=input_tensor,              # [1, 4, D, H, W] (DëŠ” ì „ì²´ ìŠ¬ë¼ì´ìŠ¤ ìˆ˜)
                    # roi_size=(128, 128, 128),        # ëª¨ë¸ì´ í•™ìŠµí•œ íŒ¨ì¹˜ í¬ê¸° (128Ã—128Ã—128)
                    # sw_batch_size=1,
                    # predictor=self.model,
                    # overlap=0.25,  # 25% overlap (ì¡°ì› ì½”ë“œì™€ ë™ì¼)
                    # mode="gaussian"  # Smooth blending (ì¡°ì› ì½”ë“œì™€ ë™ì¼)
                    # )
                    # # output: [1, 1, D, H, W] (out_channels=1ì´ë¯€ë¡œ)
                    # pred_prob = torch.sigmoid(output).squeeze(0).squeeze(0).cpu().numpy()  # [D, H, W]
                
                    # # ì„ê³„ê°’ ì¡°ì • ê°€ëŠ¥ (0.5ë³´ë‹¤ ë‚®ê²Œ ì„¤ì •í•˜ë©´ ë” ë¯¼ê°í•˜ê²Œ ê²€ì¶œ)
                    # threshold = 0.5
                    # pred_mask = (pred_prob > threshold).astype(np.uint8)
                    # logger.info(f"ğŸ“Š Output shape: {pred_mask.shape}")
                    # logger.info(f"ğŸ“Š ëª¨ë¸ ì¶œë ¥ í†µê³„: min={pred_prob.min():.4f}, max={pred_prob.max():.4f}, mean={pred_prob.mean():.4f}")
                    # logger.info(f"ğŸ“Š ë§ˆìŠ¤í¬ í†µê³„: ì´ í”½ì…€={pred_mask.size}, ì¢…ì–‘ í”½ì…€={pred_mask.sum()}, ë¹„ìœ¨={pred_mask.sum()/pred_mask.size*100:.2f}%")
            
                    # # ì „ì²´ ìŠ¬ë¼ì´ìŠ¤ í›„ì²˜ë¦¬ (ì›ë³¸ í¬ê¸° ìœ ì§€ ë˜ëŠ” ë¦¬ì‚¬ì´ì¦ˆ)
                    # logger.info(f"ğŸ“ {pred_mask.shape[0]}ê°œ ìŠ¬ë¼ì´ìŠ¤ ì „ì²´ í›„ì²˜ë¦¬ ì‹œì‘")
                    # from scipy.ndimage import zoom
            
                    # # ì›ë³¸ í¬ê¸° ê°€ì ¸ì˜¤ê¸° (4-channel ëª¨ë“œì—ì„œëŠ” original_dicomì—ì„œ, ë‹¨ì¼ ì´ë¯¸ì§€ ëª¨ë“œì—ì„œëŠ” slice_2dì—ì„œ)
                    # if original_dicom is not None:
                    # h = getattr(original_dicom, 'Rows', 256)
                    # w = getattr(original_dicom, 'Columns', 256)
                    # elif 'slice_2d' in locals() and slice_2d is not None:
                    # h, w = slice_2d.shape
                    # else:
                    # # ëª¨ë¸ ì¶œë ¥ í¬ê¸° ì‚¬ìš©
                    # h, w = pred_mask.shape[1], pred_mask.shape[2]
            
                    # # ëª¨ë¸ ì¶œë ¥ í¬ê¸° í™•ì¸
                    # model_h, model_w = pred_mask.shape[1], pred_mask.shape[2]
            
                    # # í¬ê¸°ê°€ ë‹¤ë¥´ë©´ ë¦¬ì‚¬ì´ì¦ˆ, ê°™ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    # if h != model_h or w != model_w:
                    # logger.info(f"ğŸ“ ì›ë³¸ í¬ê¸°: {h}Ã—{w}, ëª¨ë¸ ì¶œë ¥ í¬ê¸°: {model_h}Ã—{model_w} â†’ ë¦¬ì‚¬ì´ì¦ˆ í•„ìš”")
                    # zoom_factors = (h / model_h, w / model_w)
                
                    # mask_resized_3d = []
                    # for i in range(pred_mask.shape[0]):
                    # # í›„ì²˜ë¦¬ (ê²½ê³„ ì •í™•ë„ í–¥ìƒ)
                    # mask_cleaned = postprocess_mask(pred_mask[i, :, :], smooth_boundary=False)
                    # # Nearest neighborë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ê²½ê³„ ë³´ì¡´)
                    # mask_resized = zoom(mask_cleaned, zoom_factors, order=0)
                    # # ë¦¬ì‚¬ì´ì¦ˆ í›„ ì¶”ê°€ í›„ì²˜ë¦¬ (ê²½ê³„ ë¶€ë“œëŸ½ê²Œ)
                    # mask_resized = postprocess_mask(mask_resized, smooth_boundary=False)
                    # mask_resized_3d.append(mask_resized)
                    # mask_resized_3d = np.stack(mask_resized_3d, axis=0)  # [D, H, W]
                
            # ì›ë³¸ spacingìœ¼ë¡œ ë³µì› (0.5mm)
            if original_spacing is not None:
                logger.info(f"ğŸ“ ì›ë³¸ spacingìœ¼ë¡œ ë³µì›: {original_spacing} mm (í˜„ì¬: 1.5mm)")
                # scipy.ndimage.zoomìœ¼ë¡œ ì›ë³¸ í¬ê¸°ë¡œ ì§ì ‘ ë¦¬ìƒ˜í”Œë§
                from scipy.ndimage import zoom
                if original_size is not None:
                    # ì›ë³¸ í¬ê¸°ë¡œ ì§ì ‘ ë¦¬ìƒ˜í”Œë§
                    logger.info(f"ğŸ“ ì›ë³¸ í¬ê¸°ë¡œ ë³µì›: {original_size} (í˜„ì¬: {mask_resized_3d.shape})")
                    zoom_factors = [
                        original_size[0] / mask_resized_3d.shape[0],  # D
                        original_size[1] / mask_resized_3d.shape[1],  # H
                        original_size[2] / mask_resized_3d.shape[2]   # W
                    ]
                    logger.info(f"ğŸ“ Zoom factors (ì›ë³¸ í¬ê¸° ê¸°ì¤€): {zoom_factors}")
                    mask_resized_3d = zoom(mask_resized_3d, zoom_factors, order=0, mode="nearest")
                    mask_resized_3d = mask_resized_3d.astype(np.uint8)
                    logger.info(f"âœ… ì›ë³¸ í¬ê¸°ë¡œ ë³µì› ì™„ë£Œ: {mask_resized_3d.shape}")
                else:
                    # ê¸°ì¡´ ë°©ì‹: spacing ë¹„ìœ¨ë¡œ ë¦¬ìƒ˜í”Œë§
                    current_spacing = [1.5, 1.5, 1.5]  # ëª¨ë¸ ì¶œë ¥ spacing
                    zoom_factors = [
                        current_spacing[0] / original_spacing[0],
                        current_spacing[1] / original_spacing[1],
                        current_spacing[2] / original_spacing[2]
                    ]
                    logger.info(f"ğŸ“ Zoom factors (spacing ë¹„ìœ¨): {zoom_factors}")
                    mask_resized_3d = zoom(mask_resized_3d, zoom_factors, order=0, mode="nearest")
                    mask_resized_3d = mask_resized_3d.astype(np.uint8)
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                import gc
                gc.collect()
            
            # ì¤‘ì•™ ìŠ¬ë¼ì´ìŠ¤ë¥¼ ëŒ€í‘œ ì´ë¯¸ì§€ë¡œ ì‚¬ìš© (PNG ë¯¸ë¦¬ë³´ê¸°ìš©)
            center_idx = mask_resized_3d.shape[0] // 2
            mask_resized = mask_resized_3d[center_idx]
            
            # Base64 ì¸ì½”ë”©
            mask_pil = Image.fromarray((mask_resized * 255).astype(np.uint8), mode='L')
            mask_bytes = io.BytesIO()
            mask_pil.save(mask_bytes, format='PNG')
            mask_base64 = base64.b64encode(mask_bytes.getvalue()).decode('utf-8')
            
            # í†µê³„
            tumor_pixels = int(np.sum(mask_resized))
            total_pixels = int(mask_resized.size)
            tumor_ratio = float(tumor_pixels / total_pixels)
            
            # Orthancì— ì €ì¥ (ì „ì²´ ìŠ¬ë¼ì´ìŠ¤ Multi-frame DICOM SEG)
            seg_instance_id = None
            successful_slices = mask_resized_3d.shape[0]
            try:
                # Orthanc ì„¤ì • ì¶”ì¶œ
                orthanc_url = data.get('orthanc_url', ORTHANC_URL)
                orthanc_auth = data.get('orthanc_auth', (ORTHANC_USER, ORTHANC_PASSWORD))
                
                seg_series_uid = data.get('seg_series_uid')
                start_instance_number = data.get('start_instance_number', 1)
                original_series_id = data.get('original_series_id', 'unknown')
                
                dicom_seg = create_dicom_seg_multiframe(original_dicom, mask_resized_3d, seg_series_uid, start_instance_number, original_series_id)
                seg_instance_id = upload_to_orthanc(dicom_seg, orthanc_url=orthanc_url, orthanc_auth=orthanc_auth)
                logger.info(f"âœ… ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ Orthanc ì €ì¥ ì™„ë£Œ: {seg_instance_id} ({successful_slices} frames)")
            except Exception as e:
                logger.error(f"âš ï¸ Orthanc ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
            
            return {
                "success": True,
                "segmentation_mask_base64": mask_base64,
                "tumor_pixel_count": tumor_pixels,
                "total_pixel_count": total_pixels,
                "tumor_ratio_percent": tumor_ratio * 100,
                "image_size": [int(w), int(h)],
                "seg_instance_id": seg_instance_id,
                "saved_to_orthanc": seg_instance_id is not None,
                "successful_slices": successful_slices,  # ì²˜ë¦¬ëœ ìŠ¬ë¼ì´ìŠ¤ ìˆ˜
                "total_slices": mask_resized_3d.shape[0]  # ì „ì²´ ìŠ¬ë¼ì´ìŠ¤ ìˆ˜
            }
            
        except Exception as e:
            logger.error(f"ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            raise ValidationError(f"Segmentation failed: {e}")
    
    def serialize(self, data: dict) -> bytes:
        """ì‘ë‹µ ë°ì´í„° ì§ë ¬í™”"""
        import json
        return json.dumps(data).encode('utf-8')


if __name__ == "__main__":
    server = Server()
    server.append_worker(
        SegmentationWorker,
        num=1,
        max_batch_size=1,
        timeout=2400000,  # 40ë¶„ (2400ì´ˆ = 2,400,000 ë°€ë¦¬ì´ˆ)
    )
    # CLI arguments are automatically parsed by Mosec
    # max_body_size is set via --max-body-size CLI arg
    # ìµœëŒ€ body size ì„¤ì •: 500MB (ë°”ì´íŠ¸ ë‹¨ìœ„)
    server.run()
